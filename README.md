# elagnitram

This is the submission to SIAG/FME Code Quest 2023 by our team, "elagnitram".

### Gradient Descent Implementation

This README outlines the steps to run the gradient descent algorithm script `GD.py`.

### Prerequisites

Ensure you have Python installed on your system. This script is compatible with Python 3.x.

### Running the Script

To run the script, follow these steps:

1. Open a terminal or command prompt.
2. Navigate to the directory containing the `GD.py` file by using the `cd` command:

```
cd elagnitram
cd code
python GD.py
```

### Code Description

The main body of the programme begins after line 302, `if __name__ == "__main__":`. It uses the functions `generate_simulated_sample` and `gradient_descent_search_dist` to find the optimal weights.

The variable `initial_parameters` represents the weights we assigned to each pool at the beginning. It can be changed to any other guess for the weights.

The variable `max_run_time` represents the maximum number of times we run the function `gradient_descent_search_dist`.

The function `generate_simulated_sample` takes three inputs. The first is the weights of each pool, the second is the parameters given in `params.py`, and the third is the number of samples it would generate. Then it returns two outputs, with the first being the samples of the quantities of $X$ coin and $Y$ coin in each pool given the input weights and the second being a tuple containing the CVaR and the probability of larger than $\zeta$.

The function `gradient_descent_search_dist` takes six inputs. The first is the samples generated by `generate_simulated_sample`. The second is the guess weights of each pool. The third is the parameters given in `params.py`. The fourth is `learning_rate`, representing the step size. The fifth is a tuple $(\delta_1,\delta_2)$, the hyperparameters. The sixth is $\epsilon$, which determines when to stop.
